"""
Academic Data Collector for LLM Investment System
Comprehensive data collection for behavioral bias analysis and academic research
"""

import json
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import logging
from pathlib import Path

@dataclass
class BiasMetric:
    """Individual bias correction event for academic analysis"""
    timestamp: str
    decision_id: str
    bias_type: str  # 'cash_drag', 'theme_blindness', 'analysis_paralysis', 'fixed_sizing', 'no_sells'
    severity: str   # 'low', 'medium', 'high', 'critical'
    before_value: float
    after_value: float
    correction_method: str  # 'llm_retry', 'mechanical_fix', 'validation_prompt'
    llm_model: str
    success: bool
    cost_basis_points: float

@dataclass
class DecisionQualityMetric:
    """Comprehensive decision quality assessment"""
    timestamp: str
    decision_id: str
    llm_model: str
    
    # Input characteristics
    cash_percentage_input: float
    holdings_count_input: int
    available_stocks_count: int
    market_volatility: float
    
    # Decision characteristics  
    total_actions: int
    buy_actions: int
    sell_actions: int
    avg_position_size: float
    position_size_variance: float
    themes_identified: List[str]
    conviction_scores: List[float]
    
    # Bias adherence scores
    cash_deployment_score: float
    activity_level_score: float
    theme_participation_score: float
    sizing_dynamics_score: float
    sell_discipline_score: float
    
    # Overall quality
    decision_quality_score: float
    bias_adherence_score: float
    
    # Validation events
    validation_triggered: bool
    validation_attempts: int
    validation_successful: bool
    mechanical_fixes_applied: int

@dataclass  
class ThemeAnalysis:
    """Theme identification and participation analysis"""
    timestamp: str
    decision_id: str
    
    # Market themes detected
    themes_available: List[Dict[str, Any]]  # {name, momentum, stock_count, strength}
    themes_captured: List[Dict[str, Any]]   # {name, allocation, rationale}
    themes_missed: List[Dict[str, Any]]     # {name, opportunity_cost_bps}
    
    # Theme participation metrics
    total_theme_exposure: float
    strongest_theme_exposure: float
    theme_diversification_score: float
    momentum_capture_score: float

class AcademicDataCollector:
    """Comprehensive data collection system for academic research"""
    
    def __init__(self, output_dir: str = "academic_data"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Initialize data containers
        self.bias_metrics: List[BiasMetric] = []
        self.decision_metrics: List[DecisionQualityMetric] = []
        self.theme_analyses: List[ThemeAnalysis] = []
        
        # Aggregated statistics
        self.session_stats = {
            'session_id': datetime.now().strftime('%Y%m%d_%H%M%S'),
            'start_time': datetime.now().isoformat(),
            'total_decisions': 0,
            'enhancement_effectiveness': {},
            'bias_correction_rates': {},
            'llm_performance_comparison': {},
            'academic_summary': {}
        }
        
        # Setup logging
        log_file = self.output_dir / f"academic_data_{self.session_stats['session_id']}.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('AcademicDataCollector')
        
        self.logger.info("üéì Academic Data Collector initialized")
        self.logger.info(f"üìÅ Data output directory: {self.output_dir}")
    
    def record_bias_correction(self, 
                             decision_id: str,
                             bias_type: str,
                             before_value: float,
                             after_value: float,
                             correction_method: str,
                             llm_model: str,
                             success: bool,
                             severity: str = 'medium') -> None:
        """Record a behavioral bias correction event"""
        
        # Calculate cost in basis points based on empirical findings
        cost_bps_map = {
            'cash_drag': 650,
            'theme_blindness': 780, 
            'analysis_paralysis': 450,
            'fixed_sizing': 220,
            'no_sells': 120
        }
        
        base_cost = cost_bps_map.get(bias_type, 100)
        severity_multiplier = {'low': 0.5, 'medium': 1.0, 'high': 1.5, 'critical': 2.0}
        cost_bps = base_cost * severity_multiplier.get(severity, 1.0)
        
        # Adjust cost based on correction effectiveness
        if success:
            improvement_ratio = abs(after_value - before_value) / max(abs(before_value), 0.01)
            cost_bps = cost_bps * (1 - min(improvement_ratio, 0.8))  # Max 80% cost reduction
        
        bias_metric = BiasMetric(
            timestamp=datetime.now().isoformat(),
            decision_id=decision_id,
            bias_type=bias_type,
            severity=severity,
            before_value=before_value,
            after_value=after_value,
            correction_method=correction_method,
            llm_model=llm_model,
            success=success,
            cost_basis_points=cost_bps
        )
        
        self.bias_metrics.append(bias_metric)
        
        # Log for academic tracking
        self.logger.info(f"üìä BIAS CORRECTION | {bias_type.upper()} | "
                        f"Severity: {severity} | Method: {correction_method} | "
                        f"Before: {before_value:.2f} ‚Üí After: {after_value:.2f} | "
                        f"Success: {success} | Cost: {cost_bps:.0f}bps")
    
    def record_decision_quality(self, 
                              decision_id: str,
                              llm_model: str,
                              portfolio_state: Dict[str, Any],
                              market_context: Dict[str, Any],
                              decision: Dict[str, Any],
                              validation_events: Dict[str, Any]) -> None:
        """Record comprehensive decision quality metrics"""
        
        actions = decision.get('actions', [])
        buy_actions = [a for a in actions if a.get('action') == 'BUY']
        sell_actions = [a for a in actions if a.get('action') == 'SELL']
        
        # Calculate bias adherence scores
        cash_score = self._calculate_cash_deployment_score(
            portfolio_state.get('cash_percentage', 100), 
            sum(a.get('weight', 0) for a in buy_actions)
        )
        
        activity_score = self._calculate_activity_score(len(actions))
        
        theme_score = self._calculate_theme_participation_score(
            decision.get('key_themes', []),
            buy_actions
        )
        
        sizing_score = self._calculate_sizing_dynamics_score(
            [a.get('weight', 0) for a in buy_actions]
        )
        
        sell_score = self._calculate_sell_discipline_score(
            len(sell_actions),
            len(portfolio_state.get('holdings', {}))
        )
        
        # Overall scores
        bias_adherence = np.mean([cash_score, activity_score, theme_score, sizing_score, sell_score])
        decision_quality = self._calculate_overall_decision_quality(decision, actions)
        
        decision_metric = DecisionQualityMetric(
            timestamp=datetime.now().isoformat(),
            decision_id=decision_id,
            llm_model=llm_model,
            
            # Inputs
            cash_percentage_input=portfolio_state.get('cash_percentage', 0),
            holdings_count_input=len(portfolio_state.get('holdings', {})),
            available_stocks_count=len(market_context.get('available_stocks', [])),
            market_volatility=market_context.get('volatility', 0),
            
            # Decision characteristics
            total_actions=len(actions),
            buy_actions=len(buy_actions),
            sell_actions=len(sell_actions),
            avg_position_size=np.mean([a.get('weight', 0) for a in buy_actions]) if buy_actions else 0,
            position_size_variance=np.var([a.get('weight', 0) for a in buy_actions]) if len(buy_actions) > 1 else 0,
            themes_identified=decision.get('key_themes', []),
            conviction_scores=[a.get('conviction', 50) for a in actions],
            
            # Bias scores
            cash_deployment_score=cash_score,
            activity_level_score=activity_score,
            theme_participation_score=theme_score,
            sizing_dynamics_score=sizing_score,
            sell_discipline_score=sell_score,
            
            # Overall quality
            decision_quality_score=decision_quality,
            bias_adherence_score=bias_adherence,
            
            # Validation
            validation_triggered=validation_events.get('triggered', False),
            validation_attempts=validation_events.get('attempts', 0),
            validation_successful=validation_events.get('successful', True),
            mechanical_fixes_applied=validation_events.get('mechanical_fixes', 0)
        )
        
        self.decision_metrics.append(decision_metric)
        
        # Log comprehensive decision metrics
        self.logger.info(f"üéØ DECISION QUALITY | {llm_model} | ID: {decision_id} | "
                        f"Quality: {decision_quality:.1f}/100 | Bias Adherence: {bias_adherence:.1f}/100 | "
                        f"Actions: {len(actions)} ({len(buy_actions)}B/{len(sell_actions)}S) | "
                        f"Cash Deploy: {cash_score:.1f} | Activity: {activity_score:.1f} | "
                        f"Themes: {theme_score:.1f} | Sizing: {sizing_score:.1f} | Sells: {sell_score:.1f}")
    
    def record_theme_analysis(self,
                            decision_id: str,
                            themes_available: List[Dict[str, Any]],
                            themes_captured: List[Dict[str, Any]],
                            decision_actions: List[Dict[str, Any]]) -> None:
        """Record detailed theme participation analysis"""
        
        # Calculate missed opportunities
        captured_theme_names = {t['name'] for t in themes_captured}
        themes_missed = []
        
        for theme in themes_available:
            if theme['name'] not in captured_theme_names:
                # Estimate opportunity cost based on theme strength
                opportunity_cost = theme.get('momentum', 0) * theme.get('strength', 1) * 10  # Rough bps estimate
                themes_missed.append({
                    'name': theme['name'],
                    'momentum': theme.get('momentum', 0),
                    'strength': theme.get('strength', 0),
                    'opportunity_cost_bps': opportunity_cost
                })
        
        # Calculate participation metrics
        total_theme_exposure = sum(t.get('allocation', 0) for t in themes_captured)
        strongest_theme_exposure = max([t.get('allocation', 0) for t in themes_captured], default=0)
        
        theme_analysis = ThemeAnalysis(
            timestamp=datetime.now().isoformat(),
            decision_id=decision_id,
            themes_available=themes_available,
            themes_captured=themes_captured,
            themes_missed=themes_missed,
            total_theme_exposure=total_theme_exposure,
            strongest_theme_exposure=strongest_theme_exposure,
            theme_diversification_score=len(themes_captured) * 20,  # Max 100 for 5 themes
            momentum_capture_score=min(100, total_theme_exposure * 200)  # Max 100 at 50% exposure
        )
        
        self.theme_analyses.append(theme_analysis)
        
        # Log theme analysis
        captured_names = [t['name'] for t in themes_captured]
        missed_names = [t['name'] for t in themes_missed[:3]]  # Top 3 missed
        
        self.logger.info(f"üé® THEME ANALYSIS | ID: {decision_id} | "
                        f"Available: {len(themes_available)} | Captured: {len(themes_captured)} ({captured_names}) | "
                        f"Missed: {len(themes_missed)} ({missed_names}) | "
                        f"Total Exposure: {total_theme_exposure:.1%} | "
                        f"Momentum Score: {theme_analysis.momentum_capture_score:.1f}/100")
    
    def _calculate_cash_deployment_score(self, cash_pct: float, deployment_pct: float) -> float:
        """Calculate cash deployment adherence score (0-100)"""
        if cash_pct <= 15:  # Optimal range
            return 100
        elif cash_pct <= 25:  # Acceptable
            return 80 + (25 - cash_pct) * 2  # 80-100
        elif cash_pct <= 50:  # Needs improvement
            required_deployment = (cash_pct - 15) / 100
            actual_deployment = deployment_pct
            return max(20, 80 * (actual_deployment / max(required_deployment, 0.01)))
        else:  # Critical - high cash
            required_deployment = 0.30  # Should deploy at least 30%
            actual_deployment = deployment_pct
            return max(0, 50 * (actual_deployment / required_deployment))
    
    def _calculate_activity_score(self, num_actions: int) -> float:
        """Calculate trading activity score (0-100)"""
        if num_actions >= 4:
            return 100
        elif num_actions >= 2:
            return 60 + (num_actions - 2) * 20  # 60-100
        elif num_actions == 1:
            return 30
        else:
            return 0
    
    def _calculate_theme_participation_score(self, themes: List[str], buy_actions: List[Dict]) -> float:
        """Calculate theme participation score (0-100)"""
        if not themes:
            return 0
        
        # Basic scoring: 20 points per theme up to 5 themes
        theme_score = min(100, len(themes) * 20)
        
        # Bonus for actual allocation to themes (simplified)
        if buy_actions:
            total_allocation = sum(a.get('weight', 0) for a in buy_actions)
            if total_allocation >= 0.15:  # 15% minimum theme exposure
                theme_score = min(100, theme_score + 20)
        
        return theme_score
    
    def _calculate_sizing_dynamics_score(self, position_sizes: List[float]) -> float:
        """Calculate position sizing dynamics score (0-100)"""
        if not position_sizes:
            return 0
        
        if len(position_sizes) == 1:
            # Single position - check if it's reasonable size
            size = position_sizes[0]
            if 0.03 <= size <= 0.10:  # 3-10% range
                return 70
            else:
                return 40
        
        # Multiple positions - check for variance
        variance = np.var(position_sizes)
        mean_size = np.mean(position_sizes)
        
        # Score based on appropriate variance and sizing
        if variance > 0.0001:  # Some variance (not all same size)
            variance_score = 50
        else:
            variance_score = 10  # Fixed sizing penalty
        
        # Score based on reasonable average size
        if 0.03 <= mean_size <= 0.08:
            size_score = 50
        elif 0.02 <= mean_size <= 0.10:
            size_score = 30
        else:
            size_score = 10
        
        return min(100, variance_score + size_score)
    
    def _calculate_sell_discipline_score(self, num_sells: int, num_holdings: int) -> float:
        """Calculate sell discipline score (0-100)"""
        if num_holdings <= 3:
            # Small portfolio - selling not critical
            return 80 if num_sells == 0 else 100
        elif num_holdings <= 8:
            # Medium portfolio - some selling beneficial  
            if num_sells >= 1:
                return 100
            else:
                return 60
        else:
            # Large portfolio - selling important for rebalancing
            if num_sells >= 2:
                return 100
            elif num_sells >= 1:
                return 80
            else:
                return 40
    
    def _calculate_overall_decision_quality(self, decision: Dict, actions: List[Dict]) -> float:
        """Calculate overall decision quality score"""
        base_score = 50
        
        # Quality indicators
        if decision.get('market_view'):
            base_score += 10
        if decision.get('risk_assessment'):
            base_score += 10
        if len(actions) >= 2:
            base_score += 15
        if all(a.get('rationale') for a in actions):
            base_score += 10
        if any(a.get('conviction', 0) > 70 for a in actions):
            base_score += 5  # High conviction positions
        
        return min(100, base_score)
    
    def export_academic_dataset(self, filename_prefix: str = None) -> Dict[str, str]:
        """Export comprehensive dataset for academic analysis"""
        
        if filename_prefix is None:
            filename_prefix = f"llm_bias_analysis_{self.session_stats['session_id']}"
        
        exported_files = {}
        
        # 1. Bias Corrections Dataset
        if self.bias_metrics:
            bias_df = pd.DataFrame([asdict(metric) for metric in self.bias_metrics])
            bias_file = self.output_dir / f"{filename_prefix}_bias_corrections.csv"
            bias_df.to_csv(bias_file, index=False)
            exported_files['bias_corrections'] = str(bias_file)
            
            self.logger.info(f"üìä Exported {len(self.bias_metrics)} bias correction records to {bias_file}")
        
        # 2. Decision Quality Dataset
        if self.decision_metrics:
            decision_df = pd.DataFrame([asdict(metric) for metric in self.decision_metrics])
            decision_file = self.output_dir / f"{filename_prefix}_decision_quality.csv"
            decision_df.to_csv(decision_file, index=False)
            exported_files['decision_quality'] = str(decision_file)
            
            self.logger.info(f"üìä Exported {len(self.decision_metrics)} decision quality records to {decision_file}")
        
        # 3. Theme Analysis Dataset
        if self.theme_analyses:
            theme_df = pd.DataFrame([asdict(analysis) for analysis in self.theme_analyses])
            theme_file = self.output_dir / f"{filename_prefix}_theme_analysis.csv"
            theme_df.to_csv(theme_file, index=False)
            exported_files['theme_analysis'] = str(theme_file)
            
            self.logger.info(f"üìä Exported {len(self.theme_analyses)} theme analysis records to {theme_file}")
        
        # 4. Aggregated Statistics
        summary_stats = self.generate_academic_summary()
        stats_file = self.output_dir / f"{filename_prefix}_summary_statistics.json"
        with open(stats_file, 'w') as f:
            json.dump(summary_stats, f, indent=2, default=str)
        exported_files['summary_statistics'] = str(stats_file)
        
        # 5. Academic Report
        academic_report = self.generate_academic_report()
        report_file = self.output_dir / f"{filename_prefix}_academic_report.md"
        with open(report_file, 'w') as f:
            f.write(academic_report)
        exported_files['academic_report'] = str(report_file)
        
        self.logger.info(f"üéì Academic dataset export completed. Files: {list(exported_files.keys())}")
        return exported_files
    
    def generate_academic_summary(self) -> Dict[str, Any]:
        """Generate comprehensive academic summary statistics"""
        
        summary = {
            'study_metadata': {
                'session_id': self.session_stats['session_id'],
                'data_collection_period': {
                    'start': self.session_stats['start_time'],
                    'end': datetime.now().isoformat()
                },
                'total_decisions_analyzed': len(self.decision_metrics),
                'total_bias_corrections': len(self.bias_metrics),
                'total_theme_analyses': len(self.theme_analyses)
            },
            
            'bias_correction_effectiveness': {},
            'llm_performance_analysis': {},
            'behavioral_pattern_analysis': {},
            'academic_findings': {}
        }
        
        if self.bias_metrics:
            bias_df = pd.DataFrame([asdict(m) for m in self.bias_metrics])
            
            summary['bias_correction_effectiveness'] = {
                'overall_success_rate': bias_df['success'].mean(),
                'average_cost_reduction_bps': bias_df[bias_df['success']]['cost_basis_points'].mean(),
                'bias_type_breakdown': {
                    bias_type: {
                        'count': len(subset),
                        'success_rate': subset['success'].mean(),
                        'avg_cost_bps': subset['cost_basis_points'].mean()
                    }
                    for bias_type, subset in bias_df.groupby('bias_type')
                },
                'correction_method_effectiveness': {
                    method: subset['success'].mean()
                    for method, subset in bias_df.groupby('correction_method')
                }
            }
        
        if self.decision_metrics:
            decision_df = pd.DataFrame([asdict(m) for m in self.decision_metrics])
            
            summary['llm_performance_analysis'] = {
                'average_decision_quality': decision_df['decision_quality_score'].mean(),
                'average_bias_adherence': decision_df['bias_adherence_score'].mean(),
                'validation_trigger_rate': decision_df['validation_triggered'].mean(),
                'validation_success_rate': decision_df[decision_df['validation_triggered']]['validation_successful'].mean() if decision_df['validation_triggered'].sum() > 0 else 0,
                'llm_model_comparison': {
                    model: {
                        'decision_quality': subset['decision_quality_score'].mean(),
                        'bias_adherence': subset['bias_adherence_score'].mean(),
                        'validation_rate': subset['validation_triggered'].mean()
                    }
                    for model, subset in decision_df.groupby('llm_model')
                } if 'llm_model' in decision_df.columns else {}
            }
            
            summary['behavioral_pattern_analysis'] = {
                'cash_management': {
                    'avg_input_cash_pct': decision_df['cash_percentage_input'].mean(),
                    'avg_deployment_score': decision_df['cash_deployment_score'].mean(),
                    'high_cash_decisions_pct': (decision_df['cash_percentage_input'] > 50).mean()
                },
                'trading_activity': {
                    'avg_actions_per_decision': decision_df['total_actions'].mean(),
                    'avg_activity_score': decision_df['activity_level_score'].mean(),
                    'low_activity_decisions_pct': (decision_df['total_actions'] < 2).mean()
                },
                'position_sizing': {
                    'avg_position_size': decision_df['avg_position_size'].mean(),
                    'avg_sizing_score': decision_df['sizing_dynamics_score'].mean(),
                    'dynamic_sizing_rate': (decision_df['position_size_variance'] > 0.0001).mean()
                }
            }
        
        # Generate academic findings
        summary['academic_findings'] = self._generate_key_findings(summary)
        
        return summary
    
    def _generate_key_findings(self, summary: Dict) -> Dict[str, Any]:
        """Generate key academic findings from the data"""
        
        findings = {
            'hypothesis_validation': {},
            'effect_sizes': {},
            'statistical_significance': {},
            'practical_implications': {}
        }
        
        # Validate key hypotheses
        bias_effectiveness = summary.get('bias_correction_effectiveness', {})
        performance_analysis = summary.get('llm_performance_analysis', {})
        
        findings['hypothesis_validation'] = {
            'h1_cash_drag_reduction': {
                'hypothesis': 'Enhanced prompts reduce excessive cash holdings',
                'supported': bias_effectiveness.get('bias_type_breakdown', {}).get('cash_drag', {}).get('success_rate', 0) > 0.7,
                'evidence': f"Cash drag correction success rate: {bias_effectiveness.get('bias_type_breakdown', {}).get('cash_drag', {}).get('success_rate', 0):.1%}"
            },
            'h2_activity_increase': {
                'hypothesis': 'Validation system increases trading activity',
                'supported': performance_analysis.get('validation_trigger_rate', 0) > 0.3,
                'evidence': f"Validation trigger rate: {performance_analysis.get('validation_trigger_rate', 0):.1%}"
            },
            'h3_theme_capture': {
                'hypothesis': 'Enhanced prompts improve theme participation',
                'supported': len(self.theme_analyses) > 0 and np.mean([t.momentum_capture_score for t in self.theme_analyses]) > 60,
                'evidence': f"Average momentum capture score: {np.mean([t.momentum_capture_score for t in self.theme_analyses]) if self.theme_analyses else 0:.1f}/100"
            }
        }
        
        return findings
    
    def generate_academic_report(self) -> str:
        """Generate comprehensive academic report"""
        
        summary = self.generate_academic_summary()
        
        report = f"""
# LLM Investment Decision Enhancement: Behavioral Bias Correction Analysis

## Abstract

This report presents empirical analysis of behavioral bias corrections in Large Language Model (LLM) investment decision-making systems. The study implements and evaluates targeted interventions addressing five key behavioral biases identified in LLM portfolio management: cash drag, theme blindness, analysis paralysis, fixed position sizing, and insufficient selling activity.

## Methodology

### Data Collection Period
- **Session ID**: {summary['study_metadata']['session_id']}
- **Start Time**: {summary['study_metadata']['data_collection_period']['start']}
- **End Time**: {summary['study_metadata']['data_collection_period']['end']}
- **Total Decisions**: {summary['study_metadata']['total_decisions_analyzed']}
- **Bias Corrections**: {summary['study_metadata']['total_bias_corrections']}

### Enhancement Framework
The study implements a three-tier correction system:
1. **Prompt Engineering**: Enhanced prompts with bias-specific guidance
2. **Decision Validation**: Automated validation against bias thresholds  
3. **Mechanical Fixes**: Last-resort corrections when LLM fails validation

## Results

### Bias Correction Effectiveness
"""
        
        if summary.get('bias_correction_effectiveness'):
            bias_data = summary['bias_correction_effectiveness']
            report += f"""
**Overall Success Rate**: {bias_data.get('overall_success_rate', 0):.1%}
**Average Cost Reduction**: {bias_data.get('average_cost_reduction_bps', 0):.0f} basis points

#### Bias-Specific Results:
"""
            for bias_type, data in bias_data.get('bias_type_breakdown', {}).items():
                report += f"""
- **{bias_type.replace('_', ' ').title()}**: {data['count']} corrections, {data['success_rate']:.1%} success rate, {data['avg_cost_bps']:.0f} avg cost
"""
        
        if summary.get('llm_performance_analysis'):
            llm_data = summary['llm_performance_analysis']
            report += f"""
### LLM Performance Analysis
- **Average Decision Quality**: {llm_data.get('average_decision_quality', 0):.1f}/100
- **Average Bias Adherence**: {llm_data.get('average_bias_adherence', 0):.1f}/100
- **Validation Trigger Rate**: {llm_data.get('validation_trigger_rate', 0):.1%}
- **Validation Success Rate**: {llm_data.get('validation_success_rate', 0):.1%}
"""
        
        if summary.get('behavioral_pattern_analysis'):
            behavior_data = summary['behavioral_pattern_analysis']
            report += f"""
### Behavioral Pattern Analysis

#### Cash Management
- Average input cash: {behavior_data['cash_management']['avg_input_cash_pct']:.1f}%
- Deployment score: {behavior_data['cash_management']['avg_deployment_score']:.1f}/100
- High cash decisions: {behavior_data['cash_management']['high_cash_decisions_pct']:.1%}

#### Trading Activity  
- Average actions per decision: {behavior_data['trading_activity']['avg_actions_per_decision']:.1f}
- Activity score: {behavior_data['trading_activity']['avg_activity_score']:.1f}/100
- Low activity rate: {behavior_data['trading_activity']['low_activity_decisions_pct']:.1%}

#### Position Sizing
- Average position size: {behavior_data['position_sizing']['avg_position_size']:.1%}
- Sizing dynamics score: {behavior_data['position_sizing']['avg_sizing_score']:.1f}/100
- Dynamic sizing rate: {behavior_data['position_sizing']['dynamic_sizing_rate']:.1%}
"""
        
        # Add key findings
        findings = summary.get('academic_findings', {}).get('hypothesis_validation', {})
        if findings:
            report += f"""
### Hypothesis Validation

"""
            for hypothesis_id, data in findings.items():
                status = "‚úÖ SUPPORTED" if data['supported'] else "‚ùå NOT SUPPORTED"
                report += f"""
**{data['hypothesis']}**: {status}
- Evidence: {data['evidence']}

"""
        
        report += f"""
## Conclusions

The enhanced LLM investment system demonstrates measurable improvements in behavioral bias correction across multiple dimensions. Key findings include:

1. **Systematic bias identification**: Successfully detected and corrected {summary['study_metadata']['total_bias_corrections']} behavioral bias instances
2. **Validation effectiveness**: {summary.get('llm_performance_analysis', {}).get('validation_trigger_rate', 0):.1%} of decisions triggered validation, with {summary.get('llm_performance_analysis', {}).get('validation_success_rate', 0):.1%} successful self-correction
3. **Performance improvement**: Average decision quality score of {summary.get('llm_performance_analysis', {}).get('average_decision_quality', 0):.1f}/100

### Practical Implications
- LLMs can be guided to overcome systematic behavioral biases through structured prompting and validation
- The three-tier correction system balances LLM autonomy with bias prevention
- Continuous monitoring and feedback loops are essential for sustained improvement

### Future Research
- Long-term performance impact assessment
- Model-specific bias pattern analysis
- Optimization of validation thresholds
- Integration with real-time market data

---

*Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*Data Collection Framework: AcademicDataCollector v1.0*
        """
        
        return report

# Global instance for easy integration
academic_collector = AcademicDataCollector()
